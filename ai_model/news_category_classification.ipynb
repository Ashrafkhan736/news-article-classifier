{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from string import punctuation\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
    "import re\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./bbc-news-data.csv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = df.loc[:,'content'], df.loc[:,'category']\n",
    "X.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(\n",
    "    [\n",
    "        \"i\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"myself\",\n",
    "        \"we\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"ourselves\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"yourself\",\n",
    "        \"yourselves\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"himself\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"herself\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"itself\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "        \"themselves\",\n",
    "        \"what\",\n",
    "        \"which\",\n",
    "        \"who\",\n",
    "        \"whom\",\n",
    "        \"this\",\n",
    "        \"that\",\n",
    "        \"these\",\n",
    "        \"those\",\n",
    "        \"am\",\n",
    "        \"is\",\n",
    "        \"are\",\n",
    "        \"was\",\n",
    "        \"were\",\n",
    "        \"be\",\n",
    "        \"been\",\n",
    "        \"being\",\n",
    "        \"have\",\n",
    "        \"has\",\n",
    "        \"had\",\n",
    "        \"having\",\n",
    "        \"do\",\n",
    "        \"does\",\n",
    "        \"did\",\n",
    "        \"doing\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"the\",\n",
    "        \"and\",\n",
    "        \"but\",\n",
    "        \"if\",\n",
    "        \"or\",\n",
    "        \"because\",\n",
    "        \"as\",\n",
    "        \"until\",\n",
    "        \"while\",\n",
    "        \"of\",\n",
    "        \"at\",\n",
    "        \"by\",\n",
    "        \"for\",\n",
    "        \"with\",\n",
    "        \"about\",\n",
    "        \"against\",\n",
    "        \"between\",\n",
    "        \"into\",\n",
    "        \"through\",\n",
    "        \"during\",\n",
    "        \"before\",\n",
    "        \"after\",\n",
    "        \"above\",\n",
    "        \"below\",\n",
    "        \"to\",\n",
    "        \"from\",\n",
    "        \"up\",\n",
    "        \"down\",\n",
    "        \"in\",\n",
    "        \"out\",\n",
    "        \"on\",\n",
    "        \"off\",\n",
    "        \"over\",\n",
    "        \"under\",\n",
    "        \"again\",\n",
    "        \"further\",\n",
    "        \"then\",\n",
    "        \"once\",\n",
    "        \"here\",\n",
    "        \"there\",\n",
    "        \"when\",\n",
    "        \"where\",\n",
    "        \"why\",\n",
    "        \"how\",\n",
    "        \"all\",\n",
    "        \"any\",\n",
    "        \"both\",\n",
    "        \"each\",\n",
    "        \"few\",\n",
    "        \"more\",\n",
    "        \"most\",\n",
    "        \"other\",\n",
    "        \"some\",\n",
    "        \"such\",\n",
    "        \"no\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"only\",\n",
    "        \"own\",\n",
    "        \"same\",\n",
    "        \"so\",\n",
    "        \"than\",\n",
    "        \"too\",\n",
    "        \"very\",\n",
    "        \"s\",\n",
    "        \"t\",\n",
    "        \"can\",\n",
    "        \"will\",\n",
    "        \"just\",\n",
    "        \"don\",\n",
    "        \"should\",\n",
    "        \"now\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # removing non alphanumeric characters\n",
    "    text = re.sub(r\"[^A-Z a-z 0-9]\", \"\", text)\n",
    "\n",
    "    # removing punctuations\n",
    "    text = \"\".join(char for char in text if char not in punctuation)\n",
    "\n",
    "    # removing trailing whitespaces again\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "    # removing the stopword\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x:preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(),LogisticRegression())\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(y)\n",
    "X_train , X_val, y_train, y_val = train_test_split(X,labels,test_size=0.2,stratify=labels,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887325858141044"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_val)\n",
    "f1_score(y_val,y_pred,average='weighted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the whole model and save it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./category_predictor.sav']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X,y)\n",
    "filename = './category_predictor.sav'\n",
    "joblib.dump(pipe, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
